{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -q peft transformers accelerate bitsandbytes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:43:49.667171Z","iopub.execute_input":"2025-05-03T10:43:49.667336Z","iopub.status.idle":"2025-05-03T10:45:06.835527Z","shell.execute_reply.started":"2025-05-03T10:43:49.667321Z","shell.execute_reply":"2025-05-03T10:45:06.834621Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, DataCollatorForSeq2Seq,BitsAndBytesConfig\nimport json\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom peft import prepare_model_for_kbit_training,LoraConfig,get_peft_model,PeftConfig\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:45:06.837469Z","iopub.execute_input":"2025-05-03T10:45:06.837691Z","iopub.status.idle":"2025-05-03T10:45:33.995832Z","shell.execute_reply.started":"2025-05-03T10:45:06.837671Z","shell.execute_reply":"2025-05-03T10:45:33.995090Z"}},"outputs":[{"name":"stderr","text":"2025-05-03 10:45:20.630844: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746269120.835005      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746269120.896742      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def print_trainable_params(model):\n    trainable_params=0\n    all_params=0\n    for _,param in model.named_parameters():\n        all_params+=param.numel()\n        if param.requires_grad:\n            trainable_params+=param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_params} || trainable%: {100 * trainable_params / all_params}\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:45:33.996598Z","iopub.execute_input":"2025-05-03T10:45:33.997174Z","iopub.status.idle":"2025-05-03T10:45:34.001600Z","shell.execute_reply.started":"2025-05-03T10:45:33.997155Z","shell.execute_reply":"2025-05-03T10:45:34.000931Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\nfrom transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Turkish-NLP/t5-efficient-base-turkish\")\n\nbase_model = AutoModelForSeq2SeqLM.from_pretrained(\n    \"Turkish-NLP/t5-efficient-base-turkish\",\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\n\nbase_model.gradient_checkpointing_enable()\nbase_model = prepare_model_for_kbit_training(base_model)\n\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],  # Gerekirse [\"k\", \"q\", \"v\", \"o\"]\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM\n)\n\nmodel = get_peft_model(base_model, peft_config)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:49:11.994488Z","iopub.execute_input":"2025-05-03T10:49:11.995248Z","iopub.status.idle":"2025-05-03T10:49:16.400749Z","shell.execute_reply.started":"2025-05-03T10:49:11.995211Z","shell.execute_reply":"2025-05-03T10:49:16.400199Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21aa966019c948749f387bde6237b12a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/839k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cfbaad3e3c64776a8a1c8ad6afbbd39"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ndata = load_dataset(\"Renicames/turkish-law-chatbot\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:49:21.742775Z","iopub.execute_input":"2025-05-03T10:49:21.743070Z","iopub.status.idle":"2025-05-03T10:49:22.154655Z","shell.execute_reply.started":"2025-05-03T10:49:21.743050Z","shell.execute_reply":"2025-05-03T10:49:22.154134Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:45:48.120431Z","iopub.status.idle":"2025-05-03T10:45:48.120764Z","shell.execute_reply.started":"2025-05-03T10:45:48.120609Z","shell.execute_reply":"2025-05-03T10:45:48.120624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class QADataset(Dataset):\n    def __init__(self, input_texts, output_texts, tokenizer, max_length):\n        self.input_texts = input_texts\n        self.output_texts = output_texts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.input_texts)\n\n    def __getitem__(self, idx):\n        input_text = self.input_texts[idx]\n        output_text = self.output_texts[idx]\n        inputs = self.tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length)\n        outputs = self.tokenizer(output_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length)\n        input_ids = inputs['input_ids'].squeeze()\n        attention_mask = inputs['attention_mask'].squeeze()\n        labels = outputs['input_ids'].squeeze()\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:47:58.745864Z","iopub.execute_input":"2025-05-03T10:47:58.746403Z","iopub.status.idle":"2025-05-03T10:47:58.752152Z","shell.execute_reply.started":"2025-05-03T10:47:58.746377Z","shell.execute_reply":"2025-05-03T10:47:58.751363Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=\"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\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:48:02.005252Z","iopub.execute_input":"2025-05-03T10:48:02.005859Z","iopub.status.idle":"2025-05-03T10:48:02.049797Z","shell.execute_reply.started":"2025-05-03T10:48:02.005828Z","shell.execute_reply":"2025-05-03T10:48:02.049052Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_data = data['train'] \ninput_texts = [\"Soru: \" + item['Soru'] for item in train_data]\noutput_texts = [\"Cevap: \" + item['Cevap'] for item in train_data]\n\nmax_length = 512\n\ndataset = QADataset(input_texts, output_texts, tokenizer, max_length)\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\ntraining_args = TrainingArguments(\n    output_dir=\"t5-base-turkish-reincames-hukuk-qlora\",\n    num_train_epochs=5,\n    per_device_train_batch_size=8,\n    save_strategy=\"epoch\", \n    eval_strategy=\"epoch\",  \n    load_best_model_at_end=True,  \n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,  \n    save_total_limit=2,  \n    learning_rate=5e-4,\n    report_to=\"none\",\n    push_to_hub=True,\n    fp16=False,\n      label_names=[\"labels\"]\n  \n)\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:49:35.183322Z","iopub.execute_input":"2025-05-03T10:49:35.184030Z","iopub.status.idle":"2025-05-03T10:49:35.908089Z","shell.execute_reply.started":"2025-05-03T10:49:35.184004Z","shell.execute_reply":"2025-05-03T10:49:35.907344Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:52:39.083568Z","iopub.execute_input":"2025-05-03T10:52:39.084299Z","iopub.status.idle":"2025-05-03T10:52:39.088955Z","shell.execute_reply.started":"2025-05-03T10:52:39.084275Z","shell.execute_reply":"2025-05-03T10:52:39.088289Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T10:52:46.683760Z","iopub.execute_input":"2025-05-03T10:52:46.684037Z","iopub.status.idle":"2025-05-03T12:57:23.258182Z","shell.execute_reply.started":"2025-05-03T10:52:46.684016Z","shell.execute_reply":"2025-05-03T12:57:23.257546Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6680' max='6680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6680/6680 2:04:33, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.101900</td>\n      <td>1.527587</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.768400</td>\n      <td>1.395748</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.646400</td>\n      <td>1.316159</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.589800</td>\n      <td>1.271200</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.505400</td>\n      <td>1.257839</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6680, training_loss=1.7836380233307798, metrics={'train_runtime': 7474.5936, 'train_samples_per_second': 7.146, 'train_steps_per_second': 0.894, 'total_flos': 6398764913461248.0, 'train_loss': 1.7836380233307798, 'epoch': 5.0})"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"m","metadata":{}},{"cell_type":"code","source":"trainer.push_to_hub()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-03T13:01:56.745476Z","iopub.execute_input":"2025-05-03T13:01:56.745967Z","iopub.status.idle":"2025-05-03T13:02:00.208456Z","shell.execute_reply.started":"2025-05-03T13:01:56.745945Z","shell.execute_reply":"2025-05-03T13:02:00.207691Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/839k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f0e482d927a49eeba8e18d47049a39f"}},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/tuhanasinan/t5-base-turkish-reincames-hukuk-qlora/commit/1dc5c245f821978ebbf49ce0f554214f4a64b003', commit_message='End of training', commit_description='', oid='1dc5c245f821978ebbf49ce0f554214f4a64b003', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tuhanasinan/t5-base-turkish-reincames-hukuk-qlora', endpoint='https://huggingface.co', repo_type='model', repo_id='tuhanasinan/t5-base-turkish-reincames-hukuk-qlora'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":20}]}