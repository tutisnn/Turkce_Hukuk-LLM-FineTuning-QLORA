# Turkce_Hukuk-LLM-FineTuning-QLORA


---

## ğŸ›ï¸ TÃ¼rkÃ§e Hukuk LLM EÄŸitimi â€“ QLoRA ile GeliÅŸtirilmiÅŸ YaklaÅŸÄ±m

Bu proje, **2024 Teknofest Yapay Zeka YarÄ±ÅŸmasÄ± birincisi** olan hukuk tabanlÄ± dil modeli projesine katkÄ± saÄŸlayan ve bu Ã§alÄ±ÅŸmayÄ± daha verimli hale getirmeyi amaÃ§layan bir devam niteliÄŸindedir. AynÄ± veri seti ve temel model kullanÄ±larak, **QLoRA (Quantized Low-Rank Adaptation)** tekniÄŸiyle **full fine-tuning'e ek olarak** alternatif bir eÄŸitim yÃ¶ntemi Ã¶nerilmektedir.

---

### ğŸ¯ Projenin AmacÄ±

* Teknofest 2024 projesinde kullanÄ±lan hukuk verisi ve **T5 Efficient Base TÃ¼rkÃ§e modeli** ile,
* Daha kaynak verimli, esnek ve kolay eriÅŸilebilir bir eÄŸitim sÃ¼reci tasarlamak,
* QLoRA tekniÄŸi sayesinde eÄŸitim sÄ±rasÄ±nda yaÅŸanan donanÄ±m limitlerini aÅŸmak.

---

### ğŸ§  EÄŸitim SÃ¼recinden Ã‡Ä±karÄ±mlar

* Orijinal projede kullanÄ±lan `batch_size=8`, Kaggle GPU ortamÄ±nda klasik fine-tuning sÄ±rasÄ±nda **"out of memory"** hatasÄ±na yol aÃ§tÄ±.
* Bu nedenle `batch_size` deÄŸeri 2â€™ye dÃ¼ÅŸÃ¼rÃ¼lerek eÄŸitim yapÄ±lmak zorunda kalÄ±ndÄ±, fakat eÄŸitim sÃ¼resi uzadÄ± ve model performansÄ± sÄ±nÄ±rlÄ± kaldÄ±.
* QLoRA tekniÄŸi ile bu sorun ortadan kaldÄ±rÄ±ldÄ± ve `batch_size=8` tekrar kullanÄ±labilir hale geldi.
* Bu sayede eÄŸitim Ã§ok daha verimli ve kararlÄ± ÅŸekilde gerÃ§ekleÅŸtirildi.

#### ğŸ”„ Denenen QLoRA YapÄ±landÄ±rmalarÄ±

1. **Ä°lk Deneme**: `num_train_epochs = 5`
2. **Ä°kinci Deneme**: `num_train_epochs = 10`
3. **ÃœÃ§Ã¼ncÃ¼ Deneme**: `batch_size = 16`, `num_train_epochs = 15`
   â¤ `num_train_epochs` artÄ±rÄ±lmasÄ±nÄ±n nedeni: **Validation loss** hÃ¢lÃ¢ dÃ¼ÅŸmeye devam ediyordu.

#### ğŸ“Š Ã–nemli Bir GÃ¶zlem

Veri setinde **aynÄ± cevaba farklÄ± sorularla ulaÅŸan Ã¶rneklerin** bulunduÄŸu durumlarda, QLoRA ile eÄŸitilmiÅŸ modelin performansÄ± **Teknofestâ€™te kullanÄ±lan orijinal modele oldukÃ§a yakÄ±n sonuÃ§lar** vermektedir.
Bu da, QLoRAâ€™nÄ±n genelleme kabiliyetinin Ã¶zellikle **semantik Ã§eÅŸitliliÄŸi olan yapÄ±larda** gÃ¼Ã§lÃ¼ olduÄŸunu gÃ¶stermektedir.

---

### âš™ï¸ Teknik Detaylar

* **Model**: `Turkish-NLP/t5-efficient-base-turkish`
* **YÃ¶ntem**: QLoRA (4-bit quantization + LoRA adaptasyonu)
* **LoRA AyarlarÄ±**: `r=64`, `alpha=16`, `dropout=0.05`
* **KÃ¼tÃ¼phaneler**: `transformers`, `datasets`, `peft`, `bitsandbytes`, `accelerate`

---

### ğŸ“ Dosya AÃ§Ä±klamalarÄ±

| Dosya                          | AÃ§Ä±klama                                  |
| ------------------------------ | ----------------------------------------- |
| `hukuk-projesi-qlora.ipynb`    | Ä°lk QLoRA denemesi                        |
| `hukuk-projesi-qlora-v2.ipynb` | Parametre iyileÅŸtirmeleri iÃ§eren versiyon |
| `hukuk-projesi-qlora-v3.ipynb` | En verimli sonuÃ§larÄ± Ã¼reten final sÃ¼rÃ¼m   |
| `hukuk_projesi_test.ipynb`     | TÃ¼m modellerin performans karÅŸÄ±laÅŸtÄ±rmasÄ± |

---

### ğŸš€ KatkÄ±lar ve Yenilik

* Ã–nceki Teknofest projesiyle aynÄ± veri ve model temel alÄ±nmÄ±ÅŸ,
* EÄŸitim sÃ¼recine QLoRA gibi modern PEFT yÃ¶ntemleri entegre edilmiÅŸtir,
* Bu sayede klasik eÄŸitimin Ã¶nÃ¼ndeki bellek sorunlarÄ± ortadan kaldÄ±rÄ±lmÄ±ÅŸ, daha hÄ±zlÄ± ve etkili bir eÄŸitim mÃ¼mkÃ¼n kÄ±lÄ±nmÄ±ÅŸtÄ±r,
* QLoRA ile elde edilen sonuÃ§lar, Ã¶zellikle **soru Ã§eÅŸitliliÄŸi iÃ§eren senaryolarda** yÃ¼ksek baÅŸarÄ± gÃ¶stermiÅŸtir.

---


### ğŸ”— Referanslar

1. **Teknofest 2024 Hukuk Modeli GitHub Reposu**: [Teknofest Proje GitHub](https://github.com/xxxxxx)  
2. **Veri Seti - Hugging Face**: [Hukuk Veri Seti Hugging Face](https://huggingface.co/datasets/xxxxxx)  
3. **GeliÅŸtirdiÄŸim Modeller**: [Benim Model Reposu](https://github.com/xxxxxx)
---
### ğŸ“ˆ YapÄ±labilecek Ä°yileÅŸtirmeler

* EÄŸitim sayÄ±sÄ± artÄ±rÄ±labilir,
* Teknofest modelindeki gibi ROUGE deÄŸerlerine bakÄ±labilir,
* FarklÄ± LLM'ler denenebilir.

---
